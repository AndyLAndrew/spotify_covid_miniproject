{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import statements + data load\n",
    "import pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt\n",
    "spotify_dth = pd.read_csv(\"data/charts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# basic data cleaning/type conversions\n",
    "indexes = range(0, len(spotify_dth)) # index\n",
    "# converting to datetime\n",
    "spotify_dth['date'] = pd.to_datetime(spotify_dth['date'], format='%Y-%m-%d')\n",
    "\n",
    "# dropping all rows that did not make the #1 spot\n",
    "#spotify_dth = [row for index, row in tqdm(spotify_dth.iterrows(), total=len(spotify_dth)) if row['rank'] == 1]\n",
    "spotify_dth = spotify_dth.drop([index for index, row in spotify_dth.iterrows() if row['rank'] != 1])\n",
    "\n",
    "\n",
    "# dropping all rows that are not in the US\n",
    "spotify_dth_US = spotify_dth.drop([index for index, row in spotify_dth.iterrows() if row['region'] != 'United States'])\n",
    "#spotify_dth_US = [row for index, row in tqdm(spotify_dth.iterrows(), total=len(spotify_dth)) if row['region'] == 'United States']\n",
    "\n",
    "\n",
    "# resetting indeces of both\n",
    "spotify_dth.reset_index(inplace=True, drop=True)\n",
    "spotify_dth_US.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# saving to csv for reference\n",
    "spotify_dth.to_csv(\"data/number_one_all_regions_2017_2021.csv\", index=False)\n",
    "spotify_dth_US.to_csv(\"data/number_one_US_2017_2021.csv\", index=False)\n",
    "del spotify_dth # removing var to free up memory\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "# now reading all of the data from the prior project group\n",
    "years = [2017, 2018, 2019, 2020, 2021]\n",
    "spotify_dict = {}\n",
    "for year in years:\n",
    "    temp_df_name = \"spotify_\" + str(year) # concatenating df name\n",
    "    temp_df_directory = \"data/\" + str(year) + \"_complete\" + \".csv\" # concatenating directory name\n",
    "    spotify_dict[temp_df_name] = pd.read_csv(temp_df_directory) # adding df to dict\n",
    "    spotify_dict[temp_df_name].drop(['Unnamed: 0', 'Streams'], axis=1, inplace=True) # dropping index col\n",
    "    spotify_dict[temp_df_name][\"year\"] = year # adding identifying year column\n",
    "\n",
    "# concatenating into a single df\n",
    "spotify_2017_2021 = pd.concat(spotify_dict.values(), ignore_index=True)\n",
    "# dropping duplicate songs (if from the same artist)\n",
    "spotify_2017_2021.drop_duplicates(['Song Names', 'Artists'], keep='first', inplace=True)\n",
    "spotify_2017_2021.reset_index(inplace=True, drop=True) # resetting index\n",
    "# renaming song title and artist\n",
    "spotify_2017_2021.rename(columns={'Song Names': 'title', 'Artists':'artist'}, inplace=True)\n",
    "\n",
    "# merging in extra features from group's df\n",
    "spotify_merged = spotify_dth_US.merge(spotify_2017_2021[['title', 'artist', 'Danceability', 'Energy', 'Key', 'Loudness',\n",
    "                                                    'Mode', 'Speechiness', 'Acousticness', 'Instrumentalness', 'Liveness',\n",
    "                                                    'Valence', 'Tempo', 'Time Signature', 'year']], on=['title', 'artist'], how='left')\n",
    "\n",
    "# dropping NA's we acquired (aka drops rows we cannot get extra features for)\n",
    "spotify_merged.dropna(subset=['Danceability'], inplace=True)\n",
    "spotify_merged.reset_index(inplace=True, drop=True) # resetting index\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of streams included: 1257 \n",
      "length of streams dropped: 1592\n"
     ]
    }
   ],
   "source": [
    "# so with 335 NAs in streams, we have two options\n",
    "# include stream count to also account for streaming amount\n",
    "# or drop the feature and assume only the genre, danceability, etc. + the covid timeline are what matter...\n",
    "# let's split it into two dfs and simply try both\n",
    "spotify_merged_streams_incl = spotify_merged.dropna(subset=['streams'], inplace=False).copy()\n",
    "spotify_merged_streams_dropped = spotify_merged.drop(['streams'], axis=1, inplace=False).copy()\n",
    "\n",
    "# resetting index again\n",
    "spotify_merged_streams_incl.reset_index(inplace=True, drop=True)\n",
    "spotify_merged_streams_dropped.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(\"length of streams included:\",len(spotify_merged_streams_incl),\"\\nlength of streams dropped:\", len(spotify_merged_streams_dropped))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/General/lib/python3.10/site-packages/pandas/core/indexes/base.py:3652\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3651\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3652\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3653\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/anaconda3/envs/General/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/General/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[54], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Creating new column based on the dates\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m spotify_merged_streams_incl\u001B[38;5;241m.\u001B[39mloc[:, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcovid_period\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mspotify_merged_streams_incl\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdate\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mapply(map_dates_to_values)\n\u001B[1;32m     10\u001B[0m spotify_merged_streams_dropped\u001B[38;5;241m.\u001B[39mloc[:, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcovid_period\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m spotify_merged_streams_dropped[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(map_dates_to_values)\n",
      "File \u001B[0;32m~/anaconda3/envs/General/lib/python3.10/site-packages/pandas/core/frame.py:3761\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   3760\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3761\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3762\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3763\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/anaconda3/envs/General/lib/python3.10/site-packages/pandas/core/indexes/base.py:3654\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3652\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3653\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m-> 3654\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3655\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3656\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3657\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3658\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3659\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'date'"
     ]
    }
   ],
   "source": [
    "# tracking covid period\n",
    "def map_dates_to_values(date):\n",
    "    if date < pd.to_datetime('2019-03-01'):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Creating new column based on the dates\n",
    "spotify_merged_streams_incl.loc[:, 'covid_period'] = spotify_merged_streams_incl['date'].apply(map_dates_to_values)\n",
    "spotify_merged_streams_dropped.loc[:, 'covid_period'] = spotify_merged_streams_dropped['date'].apply(map_dates_to_values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-05-29T05:07:34.765488Z",
     "start_time": "2023-05-29T05:07:34.369616Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "# let's go ahead and drop the \"title\", \"rank\", \"date\", \"artist\", \"url\", \"region\", \"chart\", \"trend\", \"year\" for each\n",
    "# we could come back to this and also try fitting into an nn to get character differences too per song/artist\n",
    "# as well, we could keep the date as well and try to predict the date or simply group (unsupervised)\n",
    "spotify_merged_streams_incl.drop([\"title\", \"rank\", \"date\", \"artist\", \"url\", \"region\", \"chart\", \"trend\", \"year\"], axis=1, inplace=True)\n",
    "spotify_merged_streams_dropped.drop([\"title\", \"rank\", \"date\", \"artist\", \"url\", \"region\", \"chart\", \"trend\", \"year\"], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "# deleting unneeded dfs\n",
    "#del spotify_dth_US, spotify_2017_2021, spotify_merged"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# alright now it's time to push into the models...\n",
    "# other imports needed for ML and data cleaning\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# we can't use f1, or anything conf matrix metrics due to no probabilities\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, RocCurveDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-05-29T04:45:01.967822Z",
     "start_time": "2023-05-29T04:45:01.959724Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "spotify_merged_streams_incl = pd.read_csv('data/spotify_merged_streams_incl.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T04:45:02.239781Z",
     "start_time": "2023-05-29T04:45:02.234776Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# just performing this split on the included streams, you can do the same for dropped\n",
    "\n",
    "# training and testing splits\n",
    "# Pull Y variable out which is balance\n",
    "X = spotify_merged_streams_incl.iloc[:, 0:13].to_numpy()\n",
    "y = spotify_merged_streams_incl.iloc[:, 13:14].to_numpy().flatten()\n",
    "\n",
    "# basic T/T/Split modeling due to large size of data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-05-29T04:45:03.671296Z",
     "start_time": "2023-05-29T04:45:03.669007Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "((842, 13), (415, 13), (842,), (415,))"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debugging...\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-05-29T04:45:04.059524Z",
     "start_time": "2023-05-29T04:45:04.056918Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# my favorite frankenfunction\n",
    "\n",
    "def metrics_classification_scorer(y_train, y_test, y_pred, y_proba, show_stats = True, save_f1 = False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param y_test: your testing data\n",
    "    :param y_pred: your predictions generated from classifier\n",
    "    :param y_proba: your probabilities generated from classifier\n",
    "    :param show_stats: default is True, shows ROC Curve, and all other Metrics\n",
    "    :param save_auc: saves AUC score if desired (must write to variable)\n",
    "    :return: plots of ROC curve, Conf Matrix Metrics, and AUC Score\n",
    "    \"\"\"\n",
    "\n",
    "    label_binarizer = LabelBinarizer().fit(y_train)\n",
    "    y_onehot_test = label_binarizer.transform(y_test)\n",
    "    # y_onehot_test.shape  # (n_samples, n_classes)\n",
    "\n",
    "    if show_stats:\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_onehot_test.ravel(),\n",
    "            y_proba.ravel(),\n",
    "            name=\"micro-average OvR\",\n",
    "            color=\"darkorange\",\n",
    "        )\n",
    "        plt.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n",
    "        plt.axis(\"square\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"Micro-averaged One-vs-Rest\\nReceiver Operating Characteristic\")\n",
    "        plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.25))\n",
    "        plt.show()\n",
    "\n",
    "    # micro_roc_auc_ovr = roc_auc_score(\n",
    "    #     y_test,\n",
    "    #     y_proba,\n",
    "    #     multi_class=\"ovr\",\n",
    "    #     average=\"micro\",\n",
    "    # )\n",
    "\n",
    "    if show_stats:\n",
    "        # print(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{micro_roc_auc_ovr:.2f}\\n\")\n",
    "        print(f\"Test Set Accuracy : {accuracy_score(y_test, y_pred) * 100} %\\n\")\n",
    "        print(f\"Classification Report : \\n\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "    # saves auc score if desired\n",
    "    if save_f1: return f1_score(y_test, y_pred, average='macro')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-05-29T05:06:22.223849Z",
     "start_time": "2023-05-29T05:06:22.219160Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score when K = 1 is {53.01204819277109} and while its the max score from cross validation is 0.5632444696539989\n",
      "The accuracy score when K = 10 is {53.73493975903615} and while its the max score from cross validation is 0.6025437201907791\n",
      "The accuracy score when K = 100 is {57.10843373493976} and while its the max score from cross validation is 0.640447793956245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jl/anaconda3/envs/General/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/Users/jl/anaconda3/envs/General/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/Users/jl/anaconda3/envs/General/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "K = [1, 10, 100]\n",
    "\n",
    "accuracy_scores = []\n",
    "cv_score = []\n",
    "for k in K:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k).fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    y_prob = knn.predict_proba(X_test)\n",
    "    knn_accuracy = {accuracy_score(y_test, y_pred) * 100}\n",
    "    accuracy_scores.append(knn_accuracy)\n",
    "    scores = cross_val_score(knn, X, y, cv=10, scoring='f1_macro')\n",
    "    cv_score.append(np.max(scores))\n",
    "\n",
    "for i in range(len(accuracy_scores)):\n",
    "    print(f'The accuracy score when K = {K[i]} is {accuracy_scores[i]} and while its the max score from cross validation is {cv_score[i]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-05-29T05:09:42.048666Z",
     "start_time": "2023-05-29T05:09:41.935192Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base f1 for logit:\n",
      " 0.33386837881219905 base f1 for knn: 0.5128869760845112\n"
     ]
    }
   ],
   "source": [
    "print(\"base f1 for logit:\\n\", logit_f1, \"base f1 for knn:\", knn_f1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-05-29T04:45:23.749681Z",
     "start_time": "2023-05-29T04:45:23.742854Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
